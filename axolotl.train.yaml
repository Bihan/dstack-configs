type: task
# The name is optional, if not specified, generated randomly
name: axolotl-multi-node

# Size of the cluster
nodes: 2

image: nvcr.io/nvidia/pytorch:25.01-py3
# Required environment variables
env:
  - HF_TOKEN
  - ACCELERATE_LOG_LEVEL=info
  - WANDB_API_KEY
  - NCCL_DEBUG=INFO
  - CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
  - WANDB_NAME=axolotl-dist-llama-qlora-train
  - WANDB_PROJECT
  - HUB_MODEL_ID=meta-llama/Meta-Llama-3-70B
# Commands of the task
commands:
  # NCG container torch and flash-attn is not compatible with axolotl.
  - pip uninstall torch -y
  - pip uninstall flash-attn -y
  - pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/test/cu124
  - pip install --no-build-isolation axolotl[flash-attn,deepspeed]
  - wget https://raw.githubusercontent.com/huggingface/trl/main/examples/accelerate_configs/fsdp1.yaml
  - wget https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/examples/llama-3/qlora-fsdp-70b.yaml
  # axolotl includes hf-xet 1.1.0 which crashes while downloading, so installing the latest 1.1.2
  - pip uninstall -y hf-xet
  - pip install hf-xet --no-cache-dir
  - accelerate launch --config_file=fsdp1.yaml -m axolotl.cli.train qlora-fsdp-70b.yaml --hub-model-id $HUB_MODEL_ID --output-dir /checkpoints/qlora-llama3-70b --wandb-project $WANDB_PROJECT --wandb-name $WANDB_NAME
    --main_process_ip=$DSTACK_MASTER_NODE_IP
    --main_process_port=8008
    --machine_rank=$DSTACK_NODE_RANK
    --num_processes=$DSTACK_GPUS_NUM
    --num_machines=$DSTACK_NODES_NUM
  

resources:
  gpu:
    # 24GB or more VRAM
    memory: 80GB..
    # One or more GPU
    count: 8
  # Shared memory (for multi-gpu)
  shm_size: 24GB

volumes:
  - /checkpoints:/checkpoints
  # - /lib/x86_64-linux-gnu:/lib/x86_64-linux-gnu
  # - /lib64:/lib64
  # - /etc/libibverbs.d:/etc/libibverbs.d
  # - /etc/rdma:/etc/rdma
  # - /usr/bin/ibv_devinfo:/usr/bin/ibv_devinfo



# mkdir -p /root/.cache/huggingface/accelerate
# vim /root/.cache/huggingface/accelerate/default_config.yaml

#This works

# accelerate launch -m axolotl.cli.train examples/llama-3/qlora-fsdp-70b.yaml \
#     --main_process_ip=$DSTACK_MASTER_NODE_IP \
#     --main_process_port=8008 \
#     --machine_rank=$DSTACK_NODE_RANK \
#     --num_processes=$DSTACK_GPUS_NUM \
#     --num_machines=$DSTACK_NODES_NUM

# Looks like it works
# accelerate launch --config_file=fsdp1.yaml -m axolotl.cli.train qlora-fsdp-70b.yaml --wandb-project $WANDB_PROJECT --wandb-name $WANDB_NAME \
#     --main_process_ip=$DSTACK_MASTER_NODE_IP \
#     --main_process_port=8008 \
#     --machine_rank=$DSTACK_NODE_RANK \
#     --num_processes=$DSTACK_GPUS_NUM \
#     --num_machines=$DSTACK_NODES_NUM



# accelerate launch --config_file=fsdp1.yaml -m axolotl.cli.train qlora-fsdp-70b.yaml --hub-model-id $HUB_MODEL_ID --output-dir /checkpoints/qlora-llama3-70b --wandb-project $WANDB_PROJECT --wandb-name $WANDB_NAME \
#     --main_process_ip=$DSTACK_MASTER_NODE_IP \
#     --main_process_port=8008 \
#     --machine_rank=$DSTACK_NODE_RANK \
#     --num_processes=$DSTACK_GPUS_NUM \
#     --num_machines=$DSTACK_NODES_NUM

# # Target the active IB HCAs
# export NCCL_IB_HCA=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8

# # Infiniband = GID index 0
# export NCCL_IB_GID_INDEX=0

# # Enable peer access and GDR level for better performance
# export NCCL_NET_GDR_LEVEL=2
# export NCCL_IB_CUDA_SUPPORT=1
# export NCCL_P2P_LEVEL=SYS

# # Enable detailed debug logs
# export NCCL_DEBUG=INFO
# export NCCL_DEBUG_SUBSYS=ALL
# export NCCL_TOPO_DUMP_FILE=/tmp/nccl_topo.xml

