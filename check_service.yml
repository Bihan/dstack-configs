type: service
name: llama31

# If `image` is not specified, dstack uses its default image
python: "3.11"

env:
  - HF_TOKEN
  - MODEL_ID=meta-llama/Llama-3.1-70B-Instruct
  - MAX_MODEL_LEN=4096
commands:
  - pip install vllm
  - vllm serve $MODEL_ID
    --max-model-len $MAX_MODEL_LEN
    --tensor-parallel-size $DSTACK_GPUS_NUM
port: 8000
# (Optional) Register the model
model: meta-llama/Llama-3.1-70B-Instruct

# Uncomment to leverage spot instances
#spot_policy: auto

resources:
  gpu:
    # 24GB or more VRAM
    memory: 80GB..
    # One or more GPU
    count: 8
  # Shared memory (for multi-gpu)
  shm_size: 24GB


replicas: 1..2
scaling:
  # Requests per seconds
  metric: rps
  # Target metric value
  target: 10


volumes:
  - /root/.cache/huggingface/hub:/root/.cache/huggingface/hub