type: task
# The name is optional, if not specified, generated randomly
name: trl-train-fsdp1-distrib

# Size of the cluster
nodes: 2

image: nvcr.io/nvidia/pytorch:25.01-py3

# Required environment variables
env:
  - HF_TOKEN
  - ACCELERATE_LOG_LEVEL=info
  - WANDB_API_KEY
# Commands of the task
commands:
  - pip install transformers
  - pip install bitsandbytes
  - pip install peft
  - pip install wandb
  - git clone https://github.com/huggingface/trl
  - cd trl
  - pip install .
  - accelerate launch
    --config_file=examples/accelerate_configs/fsdp1.yaml
    --main_process_ip=$DSTACK_MASTER_NODE_IP
    --main_process_port=8008
    --machine_rank=$DSTACK_NODE_RANK
    --num_processes=$DSTACK_GPUS_NUM 
    --num_machines=$DSTACK_NODES_NUM 
    trl/scripts/sft.py
    --model_name meta-llama/Llama-3.1-8B
    --dataset_name OpenAssistant/oasst_top1_2023-08-25
    --dataset_text_field="text"
    --per_device_train_batch_size 1
    --per_device_eval_batch_size 1
    --gradient_accumulation_steps 4 
    --learning_rate 2e-4
    --report_to wandb
    --bf16
    --max_seq_length 1024
    --attn_implementation flash_attention_2 
    --logging_steps=10
    --output_dir /checkpoints/llama31-ft
    --hub_model_id sjbbihan/FineLlama-3.1-8B
    --torch_dtype bfloat16

resources:
  gpu:
    # 24GB or more VRAM
    memory: 80GB..
    # One or more GPU
    count: 8
  # Shared memory (for multi-gpu)
  shm_size: 24GB

volumes:
  - /checkpoints:/checkpoints

  #https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/fsdp_and_deepspeed.md

